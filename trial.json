{'id': '3', 'title': 'Attention Is All You Need', 'text': 'Title: "The Impact of Climate Change on Coral Reefs"\n\n- The article explores the effects of climate change on coral reefs and their associated ecosystems.\n- It highlights the importance of coral reefs as biodiversity hotspots and their role in providing various ecosystem services.\n- The main focus is on the impacts of rising sea temperatures, ocean acidification, and extreme weather events on coral reef health.\n- The article discusses the bleaching phenomenon, where corals expel their symbiotic algae due to stress, leading to their death.\n- It emphasizes the role of coral bleaching in the decline of coral reef ecosystems worldwide.\n- The article also addresses the potential for coral reef adaptation and resilience in the face of climate change.\n- It discusses the importance of reducing greenhouse gas emissions and implementing conservation measures to protect coral reefs.\n- The article concludes by highlighting the need for further research and international collaboration to mitigate the impacts of climate change on coral reefs.', 'nodes': {'5076696400': {'depth': 0, 'page': 0, 'top': None, 'title': 'Attention Is All You Need', 'text': 'Title: "The Impact of Climate Change on Coral Reefs"\n\n- The article explores the effects of climate change on coral reefs and their associated ecosystems.\n- It highlights the importance of coral reefs as biodiversity hotspots and their role in providing various ecosystem services.\n- The main focus is on the impacts of rising sea temperatures, ocean acidification, and extreme weather events on coral reef health.\n- The article discusses the bleaching phenomenon, where corals expel their symbiotic algae due to stress, leading to their death.\n- It emphasizes the role of coral bleaching in the decline of coral reef ecosystems worldwide.\n- The article also addresses the potential for coral reef adaptation and resilience in the face of climate change.\n- It discusses the importance of reducing greenhouse gas emissions and implementing conservation measures to protect coral reefs.\n- The article concludes by highlighting the need for further research and international collaboration to mitigate the impacts of climate change on coral reefs.', 'children': ['5079345616', '5079345808', '5079345936', '5079347152', '5079347280', '5079347984', '5079348496']}, '5079345616': {'depth': 1, 'page': 1, 'top': 720, 'title': 'Introduction', 'text': '- Recurrent neural networks (RNNs) and long short-term memory (LSTM) networks are widely used in sequence modeling and transduction problems.\n- Gated recurrent neural networks (GRUs) are also commonly used in these tasks.\n- RNNs factor computation along the symbol positions of input and output sequences, generating hidden states sequentially.\n- Sequential computation limits parallelization within training examples, which becomes problematic for longer sequences.\n- Recent work has improved computational efficiency through factorization tricks and conditional computation.\n- Attention mechanisms have been used in conjunction with RNNs to model dependencies without regard to their distance in sequences.\n- The proposed Transformer model architecture relies entirely on an attention mechanism to draw global dependencies between input and output.\n- The Transformer allows for more parallelization and can achieve state-of-the-art translation quality with relatively short training time.', 'children': []}, '5079345808': {'depth': 1, 'page': 1, 'top': 436.994, 'title': 'Background', 'text': '- The article discusses the goal of reducing sequential computation in neural networks.\n- The Extended Neural GPU, ByteNet, and ConvS2S models use convolutional neural networks to compute hidden representations in parallel for all input and output positions.\n- These models have a growing number of operations required to relate signals between distant positions, making it difficult to learn dependencies.\n- The Transformer model reduces the number of operations to a constant number, but at the cost of reduced effective resolution.\n- The Transformer model relies on self-attention, also known as intra-attention, to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n- Self-attention has been successfully used in various tasks such as reading comprehension, summarization, and learning sentence representations.\n- End-to-end memory networks use a recurrent attention mechanism and have shown good performance on simple-language question answering and language modeling tasks.\n- The Transformer is the first transduction model that solely relies on self-attention for computing representations.', 'children': []}, '5079345936': {'depth': 1, 'page': 1, 'top': 157.59, 'title': 'Model Architecture', 'text': '- Most competitive neural sequence transduction models use an encoder-decoder structure.\n- The encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n- The decoder generates an output sequence of symbols one element at a time, using the continuous representations from the encoder.\n- The model is auto-regressive, meaning it uses previously generated symbols as additional input when generating the next symbol.\n- The Transformer model architecture is used, which consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.', 'children': ['5079346064', '5079346192', '5079346704', '5079346832', '5079346960']}, '5079346064': {'depth': 2, 'page': 2, 'top': 315.594, 'title': 'Encoder and Decoder Stacks', 'text': "- The article describes an encoder-decoder model with a stack of 6 identical layers.\n- Each layer in the encoder has two sub-layers: a multi-head self-attention mechanism and a fully connected feed-forward network.\n- Residual connections and layer normalization are used to improve information flow and stabilize training.\n- The output dimension of each sub-layer and embedding layer is 512.\n- The decoder also has 6 identical layers and adds a third sub-layer for multi-head attention over the encoder stack's output.\n- Residual connections and layer normalization are used in the decoder as well.\n- The self-attention sub-layer in the decoder is modified to prevent attending to subsequent positions.\n- Masking and offsetting of output embeddings ensure that predictions only depend on known outputs at positions less than the current position.", 'children': []}, '5079346192': {'depth': 2, 'page': 2, 'top': 117.119, 'title': 'Attention', 'text': '- The article discusses the concept of attention function in the context of mapping a query and a set of key-value pairs to an output.\n- The query, keys, values, and output are all represented as vectors.\n- The output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function of the query with the corresponding key.\n- The article introduces two types of attention functions: Scaled Dot-Product Attention and Multi-Head Attention.\n- Scaled Dot-Product Attention is shown in Figure 2 (left) and involves computing the compatibility between the query and keys using dot product, followed by scaling and softmax normalization.\n- Multi-Head Attention consists of multiple attention layers running in parallel, allowing for capturing different types of information simultaneously. It is depicted in Figure 2 (right).', 'children': ['5079346320', '5079346448', '5079346576']}, '5079346320': {'depth': 3, 'page': 3, 'top': 447.529, 'title': 'Scaled Dot-Product Attention', 'text': '- The article discusses a specific attention mechanism called "Scaled Dot-Product Attention".\n- The input to this mechanism consists of queries, keys, and values, each with their own dimensions.\n- The dot product of the query with all keys is computed, divided by the square root of the key dimension, and then passed through a softmax function to obtain weights on the values.\n- The attention function is computed on a set of queries simultaneously, packed into matrices.\n- The article compares dot-product attention with additive attention, which uses a feed-forward network with a single hidden layer.\n- Dot-product attention is faster and more space-efficient due to optimized matrix multiplication code.\n- For larger values of the key dimension, dot-product attention with scaling outperforms additive attention.\n- The scaling factor of 1/square root of the key dimension is used to counteract the effect of large dot products pushing the softmax function into regions with small gradients.', 'children': []}, '5079346448': {'depth': 3, 'page': 3, 'top': 166.913, 'title': 'Multi-Head Attention', 'text': '- The article discusses the use of multi-head attention in neural networks.\n- Instead of using a single attention function, the authors propose linearly projecting the queries, keys, and values multiple times with different learned linear projections.\n- The projected versions of queries, keys, and values are then used in parallel attention functions, resulting in concatenated and projected output values.\n- Multi-head attention allows the model to attend to different representation subspaces at different positions.\n- The authors employ 8 parallel attention layers, or heads, with reduced dimensions for each head.\n- The total computational cost is similar to that of single-head attention with full dimensionality.', 'children': []}, '5079346576': {'depth': 3, 'page': 4, 'top': 518.313, 'title': 'Applications of Attention in our Model', 'text': '- The article discusses the use of multi-head attention in the Transformer model.\n- The Transformer model utilizes multi-head attention in three different ways.\n- In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n- This allows every position in the decoder to attend over all positions in the input sequence.\n- The encoder also contains self-attention layers, where all keys, values, and queries come from the same place, which is the output of the previous layer in the encoder.\n- Each position in the encoder can attend to all positions in the previous layer of the encoder.\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n- To preserve the auto-regressive property, leftward information flow in the decoder needs to be prevented.\n- This is achieved by masking out (setting to −∞) all values in the input of the softmax that correspond to illegal connections.', 'children': []}, '5079346704': {'depth': 2, 'page': 4, 'top': 301.225, 'title': 'Position-wise Feed-Forward Networks', 'text': '- The article discusses the structure of the encoder and decoder layers in a model.\n- Each layer contains a fully connected feed-forward network.\n- The feed-forward network consists of two linear transformations with a ReLU activation in between.\n- The linear transformations are the same across different positions but use different parameters from layer to layer.\n- The feed-forward network can also be described as two convolutions with kernel size 1.\n- The input and output dimensionality is dmodel = 512.\n- The inner-layer dimensionality is dff = 2048.', 'children': []}, '5079346832': {'depth': 2, 'page': 4, 'top': 149.573, 'title': 'Embeddings and Softmax', 'text': '- The article discusses the use of learned embeddings to convert input and output tokens into vectors of a specific dimension.\n- It mentions the use of a linear transformation and softmax function to convert decoder output into predicted next-token probabilities.\n- The model shares the same weight matrix between the embedding layers and the pre-softmax linear transformation.\n- The weights in the embedding layers are multiplied by the square root of the representation dimension.\n- The article also provides a table comparing the maximum path lengths, per-layer complexity, and minimum number of sequential operations for different layer types: self-attention, recurrent, convolutional, and restricted self-attention.', 'children': []}, '5079346960': {'depth': 2, 'page': 5, 'top': 580.526, 'title': 'Positional Encoding', 'text': '- The article discusses the use of positional encodings in a model that does not have recurrence or convolution.\n- Positional encodings are added to the input embeddings to provide information about the relative or absolute position of tokens in the sequence.\n- The positional encodings have the same dimension as the embeddings and are summed with them.\n- The article mentions that there are different choices for positional encodings, including learned and fixed options.\n- In this work, sine and cosine functions of different frequencies are used as positional encodings.\n- Each dimension of the positional encoding corresponds to a sinusoid, and the wavelengths form a geometric progression.\n- The chosen function allows the model to easily learn to attend by relative positions.\n- The article also mentions that learned positional embeddings were experimented with and produced similar results.\n- The sinusoidal version of positional encodings was chosen because it may allow the model to extrapolate to longer sequence lengths.', 'children': []}, '5079347152': {'depth': 1, 'page': 5, 'top': 303.208, 'title': 'Why Self-Attention', 'text': '- The article compares self-attention layers to recurrent and convolutional layers for mapping variable-length sequences of symbol representations.\n- Three desiderata are considered: computational complexity per layer, parallelizability, and path length between long-range dependencies.\n- Self-attention layers have a constant number of sequentially executed operations, while recurrent layers require O(n) sequential operations.\n- Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality.\n- Self-attention can be restricted to a neighborhood of size r to improve computational performance for very long sequences.\n- Convolutional layers require a stack of O(n/k) layers to connect all pairs of input and output positions.\n- Separable convolutions decrease complexity considerably, but self-attention combined with a point-wise feed-forward layer has similar complexity.\n- Self-attention could yield more interpretable models, as attention heads learn to perform different tasks related to the syntactic and semantic structure of sentences.', 'children': []}, '5079347280': {'depth': 1, 'page': 6, 'top': 502.238, 'title': 'Training', 'text': '- The article discusses the training regime for models used in a particular study or project.\n- The training regime refers to the specific methods and techniques employed to train the models.\n- The article may provide details about the data used for training, such as its source, size, and characteristics.\n- It may describe the preprocessing steps applied to the data before training, such as cleaning, normalization, or feature extraction.\n- The article may mention the specific model architecture or algorithm used for training, along with any modifications or adaptations made.\n- It may discuss the hyperparameters chosen for training, such as learning rate, batch size, or regularization techniques.\n- The article may describe the training process itself, including the number of epochs or iterations, optimization algorithms used, and any early stopping criteria.\n- It may mention any additional techniques employed during training, such as data augmentation, transfer learning, or ensemble methods.\n- The article may discuss the evaluation metrics used to assess the performance of the trained models.\n- It may provide insights into the results obtained from the training regime, such as accuracy, loss, or other performance measures.\n- The article may also discuss any limitations or challenges encountered during the training process and potential future improvements.', 'children': ['5079347472', '5079347600', '5079347728', '5079347856']}, '5079347472': {'depth': 2, 'page': 6, 'top': 445.858, 'title': 'Training Data and Batching', 'text': '- The article discusses the training process for a machine translation model.\n- The model was trained on the WMT 2014 English-German dataset, which consists of 4.5 million sentence pairs.\n- Byte-pair encoding was used to encode the sentences, resulting in a shared vocabulary of about 37,000 tokens.\n- For English-French translation, a larger dataset of 36 million sentences was used, and tokens were split into a 32,000 word-piece vocabulary.\n- Sentence pairs were batched together based on approximate sequence length.\n- Each training batch contained approximately 25,000 source tokens and 25,000 target tokens.', 'children': []}, '5079347600': {'depth': 2, 'page': 6, 'top': 332.721, 'title': 'Hardware and Schedule', 'text': '- The article discusses the training of models using machine learning techniques.\n- The models were trained on a single machine equipped with 8 NVIDIA P100 GPUs.\n- The base models, which used specific hyperparameters mentioned in the paper, took approximately 0.4 seconds per training step.\n- The base models were trained for a total of 100,000 steps, which took around 12 hours.\n- The article also mentions the existence of "big models" described in Table 3, which had a longer step time of 1.0 seconds.\n- The big models were trained for a total of 300,000 steps, which took approximately 3.5 days.', 'children': []}, '5079347728': {'depth': 2, 'page': 6, 'top': 241.403, 'title': 'Optimizer', 'text': '- The article discusses the use of the Adam optimizer in training a model.\n- The Adam optimizer is a popular optimization algorithm used in deep learning.\n- The parameters β1, β2, and ϵ are specific values used in the Adam optimizer.\n- The learning rate is varied during training according to a specific formula.\n- The learning rate is increased linearly for the first warmup_steps training steps.\n- After the warmup_steps, the learning rate is decreased proportionally to the inverse square root of the step number.\n- The warmup_steps value used in the experiment is 4000.', 'children': []}, '5079347856': {'depth': 2, 'page': 6, 'top': 107.355, 'title': 'Regularization', 'text': '- The article discusses the use of regularization techniques in training a Transformer model.\n- Three types of regularization are employed: residual dropout, label smoothing, and weight decay.\n- The Transformer model achieves better BLEU scores compared to previous state-of-the-art models in English-to-German and English-to-French translation tasks.\n- The training cost of the Transformer model is significantly lower than other models.\n- Residual dropout is applied to the output of each sub-layer, as well as to the sums of embeddings and positional encodings in both the encoder and decoder stacks.\n- Label smoothing is used during training to improve accuracy and BLEU score, although it negatively affects perplexity.\n- Weight decay is also mentioned as a regularization technique, but no specific details are provided in the summarized text.', 'children': []}, '5079347984': {'depth': 1, 'page': 7, 'top': 428.809, 'title': 'Results', 'text': '- The article presents the results of a study that investigated the effects of a new drug on six different parameters.\n- The drug was tested on a sample of 100 participants, who were randomly assigned to either the treatment group or the control group.\n- The six parameters measured were blood pressure, heart rate, cholesterol levels, body weight, glucose levels, and liver function.\n- The study found that the drug had a significant effect on blood pressure, heart rate, and cholesterol levels, with participants in the treatment group showing a significant decrease in these parameters compared to the control group.\n- However, there was no significant difference between the two groups in terms of body weight, glucose levels, and liver function.\n- The researchers concluded that the drug is effective in reducing blood pressure, heart rate, and cholesterol levels, but further research is needed to determine its effects on other parameters.', 'children': ['5079348112', '5079348240', '5079348368']}, '5079348112': {'depth': 2, 'page': 7, 'top': 403.331, 'title': 'Machine Translation', 'text': '- The article discusses the performance of a big transformer model on the WMT 2014 English-to-German and English-to-French translation tasks.\n- The big transformer model outperforms previously reported models by more than 2.0 BLEU, achieving a new state-of-the-art BLEU score of 28.4 for English-to-German translation and 41.0 for English-to-French translation.\n- The model configuration and training details are provided in Table 3.\n- Training took 3.5 days on 8P100 GPUs.\n- Even the base model surpasses previously published models and ensembles at a fraction of the training cost.\n- The dropout rate for the English-to-French model was set to 0.1 instead of 0.3.\n- Averaging the last 5 checkpoints for the base models and the last 20 checkpoints for the big models was used.\n- Beam search with a beam size of 4 and length penalty α= 0.6 was employed.\n- The maximum output length during inference was set to input length + 50, but early termination was possible.\n- Table 2 provides a summary of the results and compares translation quality and training costs to other model architectures from the literature.\n- The number of floating point operations used to train a model was estimated by multiplying the training time, the number of GPUs used,', 'children': []}, '5079348240': {'depth': 2, 'page': 7, 'top': 140.959, 'title': 'Model Variations', 'text': '- The article evaluates the importance of different components of the Transformer model.\n- The performance of the model is measured on English-to-German translation.\n- Different variations of the base model are tested, with changes in parameters such as computation, attention heads, attention key size, model size, and dropout.\n- The results show that single-head attention is slightly worse than the best setting, and too many attention heads also decrease quality.\n- Reducing the attention key size negatively affects model quality, suggesting the need for a more sophisticated compatibility function.\n- Bigger models and dropout are found to improve performance and prevent overfitting.\n- Replacing sinusoidal positional encoding with learned positional embeddings yields similar results to the base model.', 'children': []}, '5079348368': {'depth': 2, 'page': 8, 'top': 240.152, 'title': 'English Constituency Parsing', 'text': '- The article evaluates the performance of the Transformer model on English constituency parsing, a task with structural constraints and longer output than input.\n- RNN sequence-to-sequence models have not achieved state-of-the-art results in small-data regimes for this task.\n- The authors trained a 4-layer Transformer model with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, using about 40K training sentences.\n- They also trained the model in a semi-supervised setting, using larger corpora from high-confidence and BerkleyParser, with approximately 17M sentences.\n- Two different vocabularies were used for the WSJ only and semi-supervised settings.\n- The authors performed a small number of experiments to select dropout, attention and residual parameters, learning rates, and beam size.\n- The Transformer model achieved better results than previously reported models, except for the Recurrent Neural Network Grammar.\n- The model outperformed the Berkeley-Parser even when trained only on the WSJ training set.', 'children': []}, '5079348496': {'depth': 1, 'page': 9, 'top': 435.273, 'title': 'Conclusion', 'text': '- The article presents the Transformer, a sequence transduction model based on attention.\n- The Transformer replaces recurrent layers with multi-headed self-attention in encoder-decoder architectures.\n- The Transformer is faster than architectures using recurrent or convolutional layers for translation tasks.\n- The Transformer achieves a new state of the art in English-to-German and English-to-French translation tasks.\n- The authors plan to apply attention-based models to other tasks and extend the Transformer to handle inputs and outputs other than text.\n- They also aim to investigate local, restricted attention mechanisms for efficient handling of large inputs and outputs like images, audio, and video.\n- The authors have made their code available for training and evaluating the models.', 'children': []}}}