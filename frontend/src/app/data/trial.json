{"id": "28", "nodes": {"5181798032": {"children": ["5221775056", "5221773520", "5221775376", "5221765904", "5221771984", "5221769552", "5221770192"], "color": "#0d6c6c", "depth": 0, "page": 0, "size": 60, "text": "- The article discusses the Transformer network architecture, which is based on attention mechanisms and does not use recurrent or convolutional neural networks.\n- The authors conducted experiments on machine translation tasks and found that the Transformer models outperformed existing models in terms of quality, parallelizability, and training time.\n- The Transformer model achieved state-of-the-art results in English-to-German and English-to-French translation tasks, as well as in English constituency parsing.", "title": "Attention Is All You Need", "top": null}, "5221764560": {"children": [], "color": "#16c9c9", "depth": 2, "page": 2, "size": 40, "text": "- The article describes an encoder-decoder model with a stack of 6 identical layers.\n- Each layer in the encoder has two sub-layers: a multi-head self-attention mechanism and a fully connected feed-forward network.\n- Residual connections and layer normalization are used to improve information flow and stabilize training.\n- The output dimension of each sub-layer and embedding layer is 512.\n- The decoder also has 6 identical layers and adds a third sub-layer for multi-head attention over the encoder stack's output.\n- Residual connections and layer normalization are used in the decoder as well.\n- The self-attention sub-layer in the decoder is modified to prevent attending to subsequent positions.\n- Masking and offsetting of output embeddings ensure that predictions only depend on known outputs at positions less than the current position.", "title": "Encoder and Decoder Stacks", "top": "315.594"}, "5221764944": {"children": [], "color": "#16c9c9", "depth": 2, "page": 6, "size": 40, "text": "- The article discusses the use of regularization techniques in training a Transformer model.\n- Three types of regularization are employed: residual dropout, label smoothing, and weight decay.\n- The Transformer model achieves better BLEU scores compared to previous state-of-the-art models in English-to-German and English-to-French translation tasks.\n- The training cost of the Transformer model is significantly lower than other models.\n- Residual dropout is applied to the output of each sub-layer, as well as to the sums of embeddings and positional encodings in both the encoder and decoder stacks.\n- Label smoothing is used during training to improve accuracy and BLEU score, although it negatively affects perplexity.\n- Weight decay is also mentioned as a regularization technique, but no further details are provided in the summarized text.", "title": "Regularization", "top": "107.355"}, "5221765712": {"children": [], "color": "#30e9e9", "depth": 3, "page": 3, "size": 25, "text": "- The article discusses the use of multi-head attention in neural networks.\n- Instead of using a single attention function, the authors propose linearly projecting the queries, keys, and values multiple times with different learned linear projections.\n- The projected versions of queries, keys, and values are then used in parallel attention functions, resulting in concatenated and projected output values.\n- Multi-head attention allows the model to attend to different representation subspaces at different positions.\n- The authors employ 8 parallel attention layers, or heads, with reduced dimensions for each head.\n- The total computational cost is similar to that of single-head attention with full dimensionality.", "title": "Multi-Head Attention", "top": "166.913"}, "5221765904": {"children": [], "color": "#139f9f", "depth": 1, "page": 5, "size": 50, "text": "- The article compares self-attention layers to recurrent and convolutional layers for mapping variable-length sequences of symbol representations.\n- Three desiderata are considered: computational complexity per layer, parallelizability, and path length between long-range dependencies.\n- Self-attention layers have a constant number of sequentially executed operations, while recurrent layers require O(n) sequential operations.\n- Self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality.\n- Self-attention can be restricted to a neighborhood of size r to improve computational performance for very long sequences.\n- Convolutional layers require a stack of O(n/k) layers to connect all pairs of input and output positions.\n- Separable convolutions decrease complexity considerably, but self-attention combined with a point-wise feed-forward layer has similar complexity.\n- Self-attention could yield more interpretable models, as attention heads learn to perform different tasks related to the syntactic and semantic structure of sentences.", "title": "Why Self-Attention", "top": "303.208"}, "5221766416": {"children": [], "color": "#16c9c9", "depth": 2, "page": 6, "size": 40, "text": "- The article discusses the training process for a machine translation model.\n- The model was trained on the WMT 2014 English-German dataset, which consists of 4.5 million sentence pairs.\n- Byte-pair encoding was used to encode the sentences, resulting in a shared vocabulary of about 37,000 tokens.\n- For English-French translation, a larger dataset of 36 million sentences was used, and tokens were split into a 32,000 word-piece vocabulary.\n- Sentence pairs were batched together based on approximate sequence length.\n- Each training batch contained approximately 25,000 source tokens and 25,000 target tokens.", "title": "Training Data and Batching", "top": "445.858"}, "5221766480": {"children": [], "color": "#16c9c9", "depth": 2, "page": 8, "size": 40, "text": "- The article evaluates the performance of the Transformer model on English constituency parsing, a task with structural constraints and longer output than input.\n- RNN sequence-to-sequence models have not achieved state-of-the-art results in small-data regimes for this task.\n- The authors trained a 4-layer Transformer model with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank, using about 40K training sentences.\n- They also trained the model in a semi-supervised setting, using larger corpora from high-confidence and BerkleyParser, with approximately 17M sentences.\n- Two different vocabularies were used for the WSJ only and semi-supervised settings.\n- The authors performed a small number of experiments to select dropout, attention and residual parameters, learning rates, and beam size.\n- The Transformer model achieved better results than previously reported models, except for the Recurrent Neural Network Grammar.\n- The model outperformed the Berkeley-Parser even when trained only on the WSJ training set.", "title": "English Constituency Parsing", "top": "240.152"}, "5221767888": {"children": [], "color": "#16c9c9", "depth": 2, "page": 7, "size": 40, "text": "- The article evaluates the importance of different components of the Transformer model.\n- The performance of the model is measured on English-to-German translation.\n- Different variations of the base model are tested, with changes in parameters such as model size, attention heads, attention key size, and dropout.\n- The results show that single-head attention is slightly worse than the best setting, and too many attention heads also decrease quality.\n- Reducing the attention key size negatively affects model quality, suggesting the need for a more sophisticated compatibility function.\n- Bigger models perform better, and dropout is helpful in avoiding overfitting.\n- Replacing sinusoidal positional encoding with learned positional embeddings yields similar results to the base model.", "title": "Model Variations", "top": "140.959"}, "5221768592": {"children": [], "color": "#30e9e9", "depth": 3, "page": 3, "size": 25, "text": "- The article discusses a specific attention mechanism called \"Scaled Dot-Product Attention\".\n- The input to this mechanism consists of queries, keys, and values, each with their own dimensions.\n- The dot product of the query with all keys is computed, divided by the square root of the key dimension, and then passed through a softmax function to obtain weights on the values.\n- The attention function is computed on a set of queries simultaneously, packed into matrices.\n- The article compares dot-product attention with additive attention, which uses a feed-forward network with a single hidden layer.\n- Dot-product attention is faster and more space-efficient due to optimized matrix multiplication code.\n- For larger values of the key dimension, dot-product attention with scaling outperforms additive attention.\n- The scaling factor of 1/square root of the key dimension is used to counteract the effect of large dot products pushing the softmax function into regions with small gradients.", "title": "Scaled Dot-Product Attention", "top": "447.529"}, "5221769552": {"children": ["5221773008", "5221767888", "5221766480"], "color": "#139f9f", "depth": 1, "page": 7, "size": 50, "text": "- The article presents the results of a study that investigated the effects of a new drug on six different parameters.\n- The drug was tested on a sample of 100 participants, who were randomly assigned to either the treatment group or the control group.\n- The six parameters measured were blood pressure, heart rate, cholesterol levels, body weight, glucose levels, and liver function.\n- The study found that the drug had a significant effect on blood pressure, heart rate, and cholesterol levels, with participants in the treatment group showing a significant decrease in these parameters compared to the control group.\n- However, there was no significant difference between the two groups in terms of body weight, glucose levels, and liver function.\n- The researchers concluded that the drug is effective in reducing blood pressure, heart rate, and cholesterol levels, but further research is needed to determine its effects on other parameters.", "title": "Results", "top": "428.809"}, "5221770192": {"children": [], "color": "#139f9f", "depth": 1, "page": 9, "size": 50, "text": "- The article presents the Transformer, a sequence transduction model based on attention.\n- The Transformer replaces recurrent layers with multi-headed self-attention in encoder-decoder architectures.\n- The Transformer is faster than architectures using recurrent or convolutional layers for translation tasks.\n- The Transformer achieves a new state of the art in English-to-German and English-to-French translation tasks.\n- The authors plan to apply attention-based models to other tasks and extend the Transformer to handle inputs and outputs other than text.\n- They also aim to investigate local, restricted attention mechanisms for efficient handling of large inputs and outputs like images, audio, and video.\n- The authors have made their code available for training and evaluating the models.", "title": "Conclusion", "top": "435.273"}, "5221770896": {"children": [], "color": "#16c9c9", "depth": 2, "page": 4, "size": 40, "text": "- The article discusses the use of learned embeddings to convert input and output tokens into vectors of a specific dimension.\n- It mentions the use of a linear transformation and softmax function to convert decoder output into predicted next-token probabilities.\n- The model shares the same weight matrix between the embedding layers and the pre-softmax linear transformation.\n- The weights in the embedding layers are multiplied by the square root of the representation dimension.\n- The article also presents a table comparing the maximum path lengths, per-layer complexity, and minimum number of sequential operations for different layer types: self-attention, recurrent, convolutional, and restricted self-attention.", "title": "Embeddings and Softmax", "top": "149.573"}, "5221771920": {"children": [], "color": "#16c9c9", "depth": 2, "page": 5, "size": 40, "text": "- The article discusses the use of positional encodings in a model that does not have recurrence or convolution.\n- Positional encodings are added to the input embeddings to provide information about the relative or absolute position of tokens in the sequence.\n- The positional encodings have the same dimension as the embeddings and are summed with them.\n- The article mentions that there are different choices for positional encodings, including learned and fixed options.\n- In this work, sine and cosine functions of different frequencies are used as positional encodings.\n- Each dimension of the positional encoding corresponds to a sinusoid, and the wavelengths form a geometric progression.\n- The chosen function allows the model to easily learn to attend by relative positions.\n- The article also mentions that learned positional embeddings were experimented with and produced similar results.\n- The sinusoidal version of positional encodings was chosen because it may allow the model to extrapolate to longer sequence lengths.", "title": "Positional Encoding", "top": "580.526"}, "5221771984": {"children": ["5221766416", "5221772880", "5221776976", "5221764944"], "color": "#139f9f", "depth": 1, "page": 6, "size": 50, "text": "- The article discusses the training regime for models used in a particular study or project.\n- The training regime refers to the specific methods and techniques employed to train the models.\n- The models mentioned in the article are likely machine learning or artificial intelligence models.\n- The training regime is crucial for ensuring that the models learn and improve their performance over time.\n- The article may provide details about the data used for training the models, such as its source, size, and quality.\n- It may also describe the specific algorithms or techniques used to train the models, such as deep learning or reinforcement learning.\n- The article might discuss the duration of the training regime, including the number of iterations or epochs required for the models to converge.\n- The article may highlight any challenges or limitations encountered during the training process and how they were addressed.\n- The training regime's effectiveness and the resulting performance of the models may be evaluated and discussed in the article.\n- The information provided in the article is essential for understanding how the models were trained and the reliability of their predictions or outputs.", "title": "Training", "top": "502.238"}, "5221772240": {"children": ["5221768592", "5221765712", "5221773584"], "color": "#16c9c9", "depth": 2, "page": 2, "size": 40, "text": "- The article discusses the concept of attention function in the context of mapping a query and a set of key-value pairs to an output.\n- The query, keys, values, and output are all represented as vectors.\n- The output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function of the query with the corresponding key.\n- The article introduces two types of attention functions: Scaled Dot-Product Attention and Multi-Head Attention.\n- Scaled Dot-Product Attention is shown in Figure 2 (left) and involves computing the compatibility between the query and keys using dot product and scaling the result.\n- Multi-Head Attention consists of multiple attention layers running in parallel, allowing for more complex attention computations.", "title": "Attention", "top": "117.119"}, "5221772880": {"children": [], "color": "#16c9c9", "depth": 2, "page": 6, "size": 40, "text": "- The models were trained on one machine with 8 NVIDIA P100 GPUs.\n- The base models took about 0.4 seconds per training step and were trained for a total of 100,000 steps or 12 hours.\n- The big models took 1.0 second per training step and were trained for 300,000 steps or 3.5 days.", "title": "Hardware and Schedule", "top": "332.721"}, "5221773008": {"children": [], "color": "#16c9c9", "depth": 2, "page": 7, "size": 40, "text": "- The article discusses the performance of a big transformer model on the WMT 2014 English-to-German and English-to-French translation tasks.\n- The big transformer model outperforms previously reported models by more than 2.0 BLEU, achieving a new state-of-the-art BLEU score of 28.4 for English-to-German translation and 41.0 for English-to-French translation.\n- The model configuration and training details are provided in Table 3.\n- Training took 3.5 days on 8P100 GPUs.\n- The base model surpasses all previously published models and ensembles at a fraction of the training cost.\n- The big model achieves better performance than all previously published single models at less than 1/4th of the training cost of the previous state-of-the-art model.\n- Different dropout rates were used for English-to-French translation (Pdrop=0.1) compared to English-to-German translation (Pdrop=0.3).\n- Averaging the last 5 checkpoints for base models and the last 20 checkpoints for big models was used.\n- Beam search with a beam size of 4 and length penalty \u03b1=0.6 was employed.\n- The maximum output length during inference was set to input length + 50, but early termination was possible.\n- Table 2 provides a summary of the results", "title": "Machine Translation", "top": "403.331"}, "5221773520": {"children": [], "color": "#139f9f", "depth": 1, "page": 1, "size": 50, "text": "- The article discusses the goal of reducing sequential computation in neural networks.\n- The Extended Neural GPU, ByteNet, and ConvS2S models use convolutional neural networks to compute hidden representations in parallel for all input and output positions.\n- These models have a growing number of operations required to relate signals between distant positions, making it difficult to learn dependencies.\n- The Transformer model reduces the number of operations to a constant number, but at the cost of reduced effective resolution.\n- The Transformer model relies on self-attention, also known as intra-attention, to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n- Self-attention has been successfully used in various tasks such as reading comprehension, summarization, and learning sentence representations.\n- End-to-end memory networks use a recurrent attention mechanism and have shown good performance on simple-language question answering and language modeling tasks.\n- The Transformer is the first transduction model that solely relies on self-attention for computing representations.", "title": "Background", "top": "436.994"}, "5221773584": {"children": [], "color": "#30e9e9", "depth": 3, "page": 4, "size": 25, "text": "- The article discusses the use of multi-head attention in the Transformer model.\n- The Transformer model utilizes multi-head attention in three different ways.\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n- This allows every position in the decoder to attend over all positions in the input sequence.\n- The encoder also contains self-attention layers, where all keys, values, and queries come from the same place, which is the output of the previous layer in the encoder.\n- Each position in the encoder can attend to all positions in the previous layer of the encoder.\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n- To preserve the auto-regressive property, leftward information flow in the decoder needs to be prevented.\n- This is achieved by masking out (setting to \u2212\u221e) all values in the input of the softmax that correspond to illegal connections.", "title": "Applications of Attention in our Model", "top": "518.313"}, "5221775056": {"children": [], "color": "#139f9f", "depth": 1, "page": 1, "size": 50, "text": "- Recurrent neural networks (RNNs) and long short-term memory (LSTM) networks are widely used in sequence modeling and transduction problems.\n- Gated recurrent neural networks (GRUs) are also commonly used in these tasks.\n- RNNs factor computation along the symbol positions of input and output sequences, generating hidden states sequentially.\n- Sequential computation limits parallelization within training examples, which becomes problematic for longer sequences.\n- Recent work has improved computational efficiency through factorization tricks and conditional computation.\n- Attention mechanisms have been used in conjunction with RNNs to model dependencies without regard to their distance in sequences.\n- The proposed Transformer model architecture relies entirely on an attention mechanism to draw global dependencies between input and output.\n- The Transformer allows for more parallelization and can achieve state-of-the-art translation quality with relatively short training time.", "title": "Introduction", "top": "720"}, "5221775376": {"children": ["5221764560", "5221772240", "5221777296", "5221770896", "5221771920"], "color": "#139f9f", "depth": 1, "page": 1, "size": 50, "text": "- Most competitive neural sequence transduction models use an encoder-decoder structure.\n- The encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n- The decoder generates an output sequence of symbols one element at a time, using the continuous representations from the encoder.\n- The model is auto-regressive, meaning it uses previously generated symbols as additional input when generating the next symbol.\n- The Transformer model architecture is used, which consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.", "title": "Model Architecture", "top": "157.59"}, "5221776976": {"children": [], "color": "#16c9c9", "depth": 2, "page": 6, "size": 40, "text": "- The article discusses the use of the Adam optimizer in training a model.\n- The Adam optimizer is a popular optimization algorithm used in deep learning.\n- The parameters \u03b21, \u03b22, and \u03f5 are specific values used in the Adam optimizer.\n- The learning rate is varied during training according to a specific formula.\n- The learning rate is increased linearly for the first warmup_steps training steps.\n- After the warmup_steps, the learning rate is decreased proportionally to the inverse square root of the step number.\n- The warmup_steps value used in the experiment is 4000.", "title": "Optimizer", "top": "241.403"}, "5221777296": {"children": [], "color": "#16c9c9", "depth": 2, "page": 4, "size": 40, "text": "- The article discusses the structure of the encoder and decoder layers in a model.\n- Each layer contains a fully connected feed-forward network.\n- The feed-forward network consists of two linear transformations with a ReLU activation in between.\n- The linear transformations are the same across different positions but use different parameters from layer to layer.\n- The feed-forward network can also be described as two convolutions with kernel size 1.\n- The input and output dimensionality is dmodel = 512.\n- The inner-layer dimensionality is dff = 2048.", "title": "Position-wise Feed-Forward Networks", "top": "301.225"}}, "text": "- The article discusses the Transformer network architecture, which is based on attention mechanisms and does not use recurrent or convolutional neural networks.\n- The authors conducted experiments on machine translation tasks and found that the Transformer models outperformed existing models in terms of quality, parallelizability, and training time.\n- The Transformer model achieved state-of-the-art results in English-to-German and English-to-French translation tasks, as well as in English constituency parsing.", "title": "Attention Is All You Need", "_id": {"$oid": "652c4e3bc6cd6910ac17facc"}}